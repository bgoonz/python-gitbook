<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>README</title>
    <style type="text/css">
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      span.underline {
        text-decoration: underline;
      }
      div.column {
        display: inline-block;
        vertical-align: top;
        width: 50%;
      }
    </style>
    <style type="text/css">
      a.sourceLine {
        display: inline-block;
        line-height: 1.25;
      }
      a.sourceLine {
        pointer-events: none;
        color: inherit;
        text-decoration: inherit;
      }
      a.sourceLine:empty {
        height: 1.2em;
      }
      .sourceCode {
        overflow: visible;
      }
      code.sourceCode {
        white-space: pre;
        position: relative;
      }
      div.sourceCode {
        margin: 1em 0;
      }
      pre.sourceCode {
        margin: 0;
      }
      @media screen {
        div.sourceCode {
          overflow: auto;
        }
      }
      @media print {
        code.sourceCode {
          white-space: pre-wrap;
        }
        a.sourceLine {
          text-indent: -1em;
          padding-left: 1em;
        }
      }
      pre.numberSource a.sourceLine {
        position: relative;
        left: -4em;
      }
      pre.numberSource a.sourceLine::before {
        content: attr(title);
        position: relative;
        left: -1em;
        text-align: right;
        vertical-align: baseline;
        border: none;
        pointer-events: all;
        display: inline-block;
        -webkit-touch-callout: none;
        -webkit-user-select: none;
        -khtml-user-select: none;
        -moz-user-select: none;
        -ms-user-select: none;
        user-select: none;
        padding: 0 4px;
        width: 4em;
        color: #aaaaaa;
      }
      pre.numberSource {
        margin-left: 3em;
        border-left: 1px solid #aaaaaa;
        padding-left: 4px;
      }
      div.sourceCode {
      }
      @media screen {
        a.sourceLine::before {
          text-decoration: underline;
        }
      }
      code span.al {
        color: #ff0000;
        font-weight: bold;
      } /* Alert */
      code span.an {
        color: #60a0b0;
        font-weight: bold;
        font-style: italic;
      } /* Annotation */
      code span.at {
        color: #7d9029;
      } /* Attribute */
      code span.bn {
        color: #40a070;
      } /* BaseN */
      code span.bu {
      } /* BuiltIn */
      code span.cf {
        color: #007020;
        font-weight: bold;
      } /* ControlFlow */
      code span.ch {
        color: #4070a0;
      } /* Char */
      code span.cn {
        color: #880000;
      } /* Constant */
      code span.co {
        color: #60a0b0;
        font-style: italic;
      } /* Comment */
      code span.cv {
        color: #60a0b0;
        font-weight: bold;
        font-style: italic;
      } /* CommentVar */
      code span.do {
        color: #ba2121;
        font-style: italic;
      } /* Documentation */
      code span.dt {
        color: #902000;
      } /* DataType */
      code span.dv {
        color: #40a070;
      } /* DecVal */
      code span.er {
        color: #ff0000;
        font-weight: bold;
      } /* Error */
      code span.ex {
      } /* Extension */
      code span.fl {
        color: #40a070;
      } /* Float */
      code span.fu {
        color: #06287e;
      } /* Function */
      code span.im {
      } /* Import */
      code span.in {
        color: #60a0b0;
        font-weight: bold;
        font-style: italic;
      } /* Information */
      code span.kw {
        color: #007020;
        font-weight: bold;
      } /* Keyword */
      code span.op {
        color: #666666;
      } /* Operator */
      code span.ot {
        color: #007020;
      } /* Other */
      code span.pp {
        color: #bc7a00;
      } /* Preprocessor */
      code span.sc {
        color: #4070a0;
      } /* SpecialChar */
      code span.ss {
        color: #bb6688;
      } /* SpecialString */
      code span.st {
        color: #4070a0;
      } /* String */
      code span.va {
        color: #19177c;
      } /* Variable */
      code span.vs {
        color: #4070a0;
      } /* VerbatimString */
      code span.wa {
        color: #60a0b0;
        font-weight: bold;
        font-style: italic;
      } /* Warning */
    </style>
  </head>
  <body>
    <h1 id="general-data-structures-notes">General Data Structures Notes</h1>
    <p>
      <a
        href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#data-types"
        >Data Types</a
      >
    </p>
    <ul>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#data-structures"
          >Data Structures</a
        >
      </li>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#array"
          >Array</a
        >
      </li>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#linked-list"
          >Linked List</a
        >
      </li>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#tree"
          >Tree</a
        >
        <ul>
          <li>
            <a
              href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#binary-tree"
              >Binary Tree</a
            >
          </li>
          <li>
            <a
              href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#binary-search-tree"
              >Binary Search Tree</a
            >
          </li>
          <li>
            <a
              href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#red-black-tree"
              >Red-Black Tree</a
            >
          </li>
          <li>
            <a
              href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#b-tree"
              >B-tree</a
            >
          </li>
          <li>
            <a
              href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#weight-balanced-tree"
              >Weight-balanced Tree</a
            >
          </li>
          <li>
            <a
              href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#binary-heap"
              >Binary Heap</a
            >
          </li>
        </ul>
      </li>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#hash-table"
          >Hash Table</a
        >
      </li>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#graph"
          >Graph</a
        >
      </li>
      <li>
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#conclusion"
          >Conclusion</a
        >
      </li>
    </ul>
    <p>
      In this post we will be looking briefly at, and at a high-level, the
      various data types and data structures used in designing software systems,
      and from which specific types of algorithms can subsequently be built upon
      and optimized for.
    </p>
    <p>
      There are <em>many</em> data structures, and even the ones that are
      covered here have many nuances that make it impossible to cover every
      possible detail. But my hope is that this will give you an interest to
      research them further.
    </p>
    <p>
      <strong>But before we get into it‚Ä¶</strong> time for some self-promotion
      üôä<a href="https://leanpub.com/pythonforprogrammers"
        >Want to be up and running <strong>quickly</strong> with
        <strong>Python</strong>? Get started here with my book ‚Äú<strong
          >Python for Programmers</strong
        >‚Äù!</a
      >
    </p>
    <h3 id="data-types">Data Types <a id="data-types"></a></h3>
    <p>
      A data type is an
      <a href="https://english.stackexchange.com/a/28098/334144">attribute</a>
      of data which tells the compiler (or interpreter) how the programmer
      intends to use the data.
    </p>
    <ul>
      <li>
        <strong>Scalar</strong>: basic building block (boolean, integer, float,
        char etc.)
      </li>
      <li>
        <strong>Composite</strong>: any data type (struct, array, string etc.)
        composed of scalars or composite types (also referred to as a ‚Äòcompound‚Äô
        type).
      </li>
      <li>
        <strong>Abstract</strong>: data type that is defined by its behaviour
        (tuple, set, stack, queue, graph etc).
      </li>
    </ul>
    <blockquote>
      <p>
        <strong>NOTE</strong>: You might also have heard of a ‚Äòprimitive‚Äô type,
        which is sometimes confused with the ‚Äòscalar‚Äô type. A primitive is
        typically used to represent a ‚Äòvalue type‚Äô (e.g.¬†pass-by-value
        semantics) and this contrasts with ‚Äòreference types‚Äô
        (e.g.¬†pass-by-reference semantics).
      </p>
    </blockquote>
    <p>
      If we consider a composite type, such as a ‚Äòstring‚Äô, it
      <em>describes</em> a data structure which contains a sequence of char
      scalars (characters), and as such is referred to as being a ‚Äòcomposite‚Äô
      type. Whereas the underlying <em>implementation</em> of the string
      composite type is typically implemented using an array data structure
      (we‚Äôll cover
      <a
        href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#data-structures"
        >data structures</a
      >
      shortly).
    </p>
    <blockquote>
      <p>
        Note: in a language like C the length of the string‚Äôs underlying array
        will be the number of characters in the string followed by a ‚Äò<a
          href="https://www.integralist.co.uk/posts/concepts-from-the-c-programming-language/#7"
          >null terminator</a
        >‚Äô.
      </p>
    </blockquote>
    <p>
      An abstract data type (ADT) describes the expected
      <em>behaviour</em> associated with a concrete data structure. For example,
      a ‚Äòlist‚Äô is an abstract data type which represents a countable number of
      ordered values, but again the <em>implementation</em> of such a data type
      could be implemented using a variety of different data structures, one
      being a ‚Äò<a href="https://en.wikipedia.org/wiki/Linked_list"
        >linked list</a
      >‚Äô.
    </p>
    <blockquote>
      <p>
        Note: an ADT describes behaviour from the perspective of a consumer of
        that type (e.g.¬†it describes certain operations that can be performed on
        the data itself). For example, a list data type can be considered a
        sequence of values and so one available operation/behaviour would be
        that it must be iterable.
      </p>
    </blockquote>
    <h3 id="data-structures">Data Structures <a id="data-structures"></a></h3>
    <p>
      A data structure is a collection of data type ‚Äòvalues‚Äô which are stored
      and organized in such a way that it allows for efficient access and
      modification. In some cases a data structure can become the underlying
      implementation for a particular data type.
    </p>
    <p>
      For example, composite data types are data structures that are composed of
      scalar data types and/or other composite types, whereas an abstract data
      type will define a set of behaviours (almost like an ‚Äòinterface‚Äô in a
      sense) for which a particular data structure can be used as the concrete
      implementation for that data type.
    </p>
    <p>When we think of data structures, there are generally four forms:</p>
    <ol type="1">
      <li><strong>Linear</strong>: arrays, lists</li>
      <li><strong>Tree</strong>: binary, heaps, space partitioning etc.</li>
      <li><strong>Hash</strong>: distributed hash table, hash tree etc.</li>
      <li><strong>Graphs</strong>: decision, directed, acyclic etc.</li>
    </ol>
    <blockquote>
      <p>
        Note: for a more complete reference, please see this
        <a href="https://en.wikipedia.org/wiki/List_of_data_structures"
          >Wikipedia article</a
        >.
      </p>
    </blockquote>
    <p>
      Let‚Äôs now take a look at the properties that make up a few of the more
      well known data structures.
    </p>
    <h3 id="array">Array <a id="array"></a></h3>
    <p>
      An array is a finite group of data, which is allocated contiguous
      (i.e.¬†sharing a common border) memory locations, and each element within
      the array is accessed via an index key (typically numerical, and zero
      based).
    </p>
    <p>
      The name assigned to an array is typically a pointer to the first item in
      the array. Meaning that given an array identifier of
      <code>arr</code> which was assigned the value
      <code>["a", "b", "c"]</code>, in order to access the
      <code>"b"</code> element you would use the index 1 to lookup the value:
      <code>arr[1]</code>.
    </p>
    <p>
      Arrays are traditionally ‚Äòfinite‚Äô in size, meaning you define their
      length/size (i.e.¬†memory capacity) up front, but there is a concept known
      as ‚Äòdynamic arrays‚Äô (and of which you‚Äôre likely more familiar with when
      dealing with certain high-level programmings languages) which supports the
      <em>growing</em> (or resizing) of an array to allow for more elements to
      be added to it.
    </p>
    <p>
      In order to resize an array you first need to allocate a new slot of
      memory (in order to copy the original array element values over to), and
      because this type of operation is quite ‚Äòexpensive‚Äô (in terms of
      computation and performance) you need to be sure you increase the memory
      capacity just the right amount (typically double the original size) to
      allow for more elements to be added at a later time without causing the
      CPU to have to resize the array over and over again unnecessarily.
    </p>
    <p>
      One consideration that needs to be given is that you don‚Äôt want the
      resized memory space to be <em>too</em> large, otherwise finding an
      appropriate slot of memory becomes more tricky.
    </p>
    <p>
      When dealing with modifying arrays you also need to be careful because
      this requires significant overhead due to the way arrays are allocated
      memory slots.
    </p>
    <p>
      So if you imagine you have an array and you want to remove an element from
      the middle of the array, try to think about that in terms of memory
      allocation: an array needs its indexes to be contiguous, and so we have to
      re-allocate a new chunk of memory and copy over the elements that were
      placed <em>around</em> the deleted element.
    </p>
    <p>
      These types of operations, when done at scale, are the foundation behind
      why it‚Äôs important to have an understanding of how data structures are
      implemented. The reason being, when you‚Äôre writing an algorithm you will
      hopefully be able to recognize when you‚Äôre about to do something (let‚Äôs
      say modify an array many times within a loop construct) that could
      ultimately end up being quite a memory intensive set of operations.
    </p>
    <blockquote>
      <p>
        Note: interestingly I‚Äôve discovered that in some languages an array (as
        in the composite data type) has been implemented using a variety of
        different data structures such as hash table, linked list, and even a
        search tree.
      </p>
    </blockquote>
    <h3 id="linked-list">Linked List <a id="linked-list"></a></h3>
    <p>
      A linked list is different to an array in that the order of the elements
      within the list are not determined by a contiguous memory allocation.
      Instead the elements of the linked list can be sporadically placed in
      memory due to its design, which is that each element of the list (also
      referred to as a ‚Äònode‚Äô) consists of two parts:
    </p>
    <ol type="1">
      <li>the data</li>
      <li>a pointer</li>
    </ol>
    <p>
      The data is what you‚Äôve assigned to that element/node, whereas the pointer
      is a memory address reference to the next node in the list.
    </p>
    <p><img src="https://www.integralist.co.uk/images/linked-list.png" /></p>
    <p>
      Also unlike an array, there is no index access. So in order to locate a
      specific piece of data you‚Äôll need to traverse the entire list until you
      find the data you‚Äôre looking for.
    </p>
    <p>
      This is one of the key performance characteristics of a linked list, and
      is why (for most implementations of this data structure) you‚Äôre not able
      to <em>append</em> data to the list (because if you think about the
      performance of such an operation it would require you to traverse the
      entire list to find the end/last node). Instead linked lists generally
      will only allow <em>prepending</em> to a list as it‚Äôs much quicker. The
      newly added node will then have its pointer set to the original ‚Äòhead‚Äô of
      the list.
    </p>
    <p>
      There is also a modified version of this data structure referred to as a
      ‚Äòdoubly linked list‚Äô which is essentially the same concept but with the
      exception of a third attribute for each node: a pointer to the
      <em>previous</em> node (whereas a normal linked list would only have a
      pointer to the <em>following</em> node).
    </p>
    <blockquote>
      <p>
        Note: again, performance considerations need to be given for the types
        of operations being made with a doubly linked list, such as the addition
        or removal of nodes in the list, because you now have not only the
        pointers to the following node that need to be updated, but also the
        pointers back to a previous node that now also need to be updated.
      </p>
    </blockquote>
    <h3 id="tree">Tree <a id="tree"></a></h3>
    <p>
      The concept of a ‚Äòtree‚Äô in its simplest terms is to represent a
      hierarchical tree structure, with a root value and subtrees of children
      (with a parent node), represented as a set of linked nodes.
    </p>
    <p><img src="https://www.integralist.co.uk/images/tree.png" /></p>
    <p>
      A tree contains ‚Äúnodes‚Äù (a node has a value associated with it) and each
      node is connected by a line called an ‚Äúedge‚Äù. These lines represent the
      <em>relationship</em> between the nodes.
    </p>
    <p>
      The top level node is known as the ‚Äúroot‚Äù and a node with no children is a
      ‚Äúleaf‚Äù. If a node is connected to other nodes, then the preceeding node is
      referred to as the ‚Äúparent‚Äù, and nodes following it are ‚Äúchild‚Äù nodes.
    </p>
    <p>
      There are various incarnations of the basic tree structure, each with
      their own unique characteristics and performance considerations:
    </p>
    <ul>
      <li>Binary Tree</li>
      <li>Binary Search Tree</li>
      <li>Red-Black Tree</li>
      <li>B-tree</li>
      <li>Weight-balanced Tree</li>
      <li>Heap</li>
      <li>Abstract Syntax Tree</li>
    </ul>
    <h4 id="binary-tree">Binary Tree <a id="binary-tree"></a></h4>
    <p>
      A binary tree is a ‚Äòrooted tree‚Äô and consists of nodes which have, at
      most, two children. This is as the name suggests (i.e.¬†‚Äòbinary‚Äô: 0 or 1),
      so <em>two</em> potential values/directions.
    </p>
    <p>
      Rooted trees suggest a notion of <em>distance</em> (i.e.¬†distance from the
      ‚Äòroot‚Äô node)
    </p>
    <blockquote>
      <p>
        Note: in some cases you might refer to a binary tree as an ‚Äòundirected‚Äô
        graph (we‚Äôll look at
        <a
          href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#graph"
          >graphs</a
        >
        shortly) if talking in the context of graph theory or mathematics.
      </p>
    </blockquote>
    <p>
      Binary trees are the building blocks of <em>other</em> tree data
      structures (see also:
      <a href="https://stackoverflow.com/a/2200588/4288305">this reference</a>
      for more details), and so when it comes to the performance of certain
      operations (insertion, deletion etc) consideration needs to be given to
      the number of ‚Äòhops‚Äô that need to be made as well as the re-balancing of
      the tree (much the same way as the pointers for a linked list need to be
      updated).
    </p>
    <h4 id="binary-search-tree">
      Binary Search Tree <a id="binary-search-tree"></a>
    </h4>
    <p>
      A binary search tree is a ‚Äòsorted‚Äô tree, and is named as such because it
      helps to support the use of a ‚Äòbinary search‚Äô algorithm for searching more
      efficiently for a particular node (more on that later).
    </p>
    <p>
      <img src="https://www.integralist.co.uk/images/binary-search-tree.png" />
    </p>
    <p>
      To understand the idea of the nodes being ‚Äòsorted‚Äô (or ‚Äòordered‚Äô) we need
      to compare the left node with the right node. The left node should always
      be a lesser number than the right node, and the parent node should be the
      decider as to whether a child node is placed to the left or the right.
    </p>
    <p>
      Consider the example image above, where we can see the root node is
      <code>8</code>. Let‚Äôs imagine we‚Äôre going to construct this tree.
    </p>
    <p>
      We start with <code>8</code> as the root node and then we‚Äôre given the
      number <code>3</code> to insert into the tree. At this point the
      underlying logic for constructing the tree will know that the number
      <code>3</code> is <em>less</em> than <code>8</code> and so it‚Äôll first
      check to see if there is already a left node (there isn‚Äôt), so in this
      scenario the logic will determine that the tree should have a new left
      node under <code>8</code> and assign it the value of <code>3</code>.
    </p>
    <p>
      Now if we give the number <code>6</code> to be inserted, the logic will
      find that again it is less than <code>8</code> and so it‚Äôll check for a
      left node. There is a left node (it has a value of <code>3</code>) and so
      the value <code>6</code> is <em>greater</em> than <code>3</code>. This
      means the logic will now check to see if there is a right node (there
      isn‚Äôt) and subsequently creates a new right node and assigns it the value
      <code>6</code>.
    </p>
    <p>
      This process continues on and on until the tree has been provided all of
      the relevant numbers to be sorted.
    </p>
    <p>
      In essence what this sorted tree design facilitates is the means for an
      operation (such as lookup, insertion, deletion) to only take, on average,
      time proportional to the
      <a href="https://en.wikipedia.org/wiki/Logarithm">logarithm</a> of the
      number of items stored in the tree.
    </p>
    <p>
      So if there were 1000 nodes in the tree, and we wanted to find a specific
      node, then the average case number of comparisons (i.e.¬†comparing
      left/right nodes) would be <code>10</code>.
    </p>
    <p>
      By using the logarithm to calculate this we get:
      <code>log 2(10) = 1024</code> which is the inverse of the exponentiation
      <code>2^10</code> (‚Äú2 raised to the power of 10‚Äù), so this says we‚Äôll
      execute 10 comparisons before finding the node we were after.
    </p>
    <p>
      To break that down a bit further: the exponentiation calculation is
      <code>1024 = 2 √ó 2 √ó 2 x 2 x 2 x 2 √ó 2 √ó 2 x 2 x 2 = 2^10</code>, so the
      ‚Äúlogarithm to base 2‚Äù of 10 is 1024.
    </p>
    <p>
      The logarithm (i.e.¬†the inverse function of exponentiation) of 1000 to
      base 2, in this case abstracted to <code>n</code>, is denoted as
      <code>log 2 (n)</code>, but typically the base 2 is omitted to just
      <code>log(n)</code>.
    </p>
    <p>
      When determining the ‚Äòtime complexity‚Äô for operations on this type of data
      structure we typically use ‚ÄòBig O‚Äô notation and thus the Big O complexity
      would be defined as <code>O(log n)</code> for the average search case
      (which is good), but the <em>worst case</em> for searching would still be
      <code>O(n)</code> linear time (which is bad ‚Äì and I‚Äôll explain why in the
      next section on
      <a
        href="https://www.integralist.co.uk/posts/data-types-and-data-structures/#red-black-tree"
        >red-black trees</a
      >).
    </p>
    <blockquote>
      <p>
        Note: I‚Äôve covered the basics of logarithm and binary search in a
        <a href="https://www.integralist.co.uk/posts/big-o-for-beginners/#8"
          >much older post</a
        >
        about Big O notation, and so I‚Äôll refer you to that for more details.
      </p>
    </blockquote>
    <p>
      Similarly when considering complexity for a particular algorithm, we
      should take into account both ‚Äòtime‚Äô and ‚Äòspace‚Äô complexity. The latter is
      the amount of memory necessary for the algorithm to execute and is similar
      to time complexity in that we‚Äôre interested in how that resource (time vs
      space) will change and affect the performance depending on the size of the
      input.
    </p>
    <h4 id="red-black-tree">Red-Black Tree <a id="red-black-tree"></a></h4>
    <p>
      The performance of a binary search tree is dependant on the height of the
      tree. Meaning we should aim to keep the tree as ‚Äòbalanced‚Äô as possible,
      otherwise the logarithm performance is lost in favor of linear time.
    </p>
    <p>
      To understand why that is, consider the following data stored in an array:
    </p>
    <div class="sourceCode" id="cb1">
      <pre
        class="sourceCode python"
      ><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1">[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]</a></code></pre>
    </div>
    <p>
      If we construct a binary search tree from this data, what we would
      ultimately end up with is a very ‚Äòunbalanced‚Äô tree in the sense that all
      the nodes would be to the right, and none to the left.
    </p>
    <p><img src="https://www.integralist.co.uk/images/bst-unbalanced.png" /></p>
    <p>
      When we search this type of tree (which for all purposes is effectively a
      linked list) we would, worst case, end up with linear time complexity:
      <code>O(n)</code>. To resolve that problem we need a way to balance the
      nodes in the tree.
    </p>
    <p>
      This is where the concept of a red-black tree comes in to help us. With a
      red-black tree (due to it being consistently balanced) we get
      <code>O(log n)</code> for search/insert/delete operations (which is
      great).
    </p>
    <p>Let‚Äôs consider the properties of a red-black tree:</p>
    <ul>
      <li>Each node is either red or black.</li>
      <li>The root node is always black.</li>
      <li>All leaves are ‚ÄòNIL‚Äô and should also be black.</li>
      <li>All red nodes should have two black child nodes.</li>
      <li>
        All paths from given node to NIL must have same num of black nodes.
      </li>
      <li>New nodes should be red by default (we‚Äôll clarify below).</li>
    </ul>
    <blockquote>
      <p>
        Note: when counting nodes we don‚Äôt include the root node, and we count
        each black node up to (and including) the NIL node.
      </p>
    </blockquote>
    <p><img src="https://www.integralist.co.uk/images/red-black-tree.png" /></p>
    <p>
      The height of the tree is referred to as its ‚Äòblack-height‚Äô, which is the
      number of black nodes (not including the root) to the furthest leaf, and
      should be no longer than twice as long as the length of the shortest path
      (the nearest NIL).
    </p>
    <p>
      These properties are what enable the red-black tree to provide the
      performance characteristics it has (i.e.¬†<code>O(log n)</code>), and so
      whenever changes are made to the tree we want to aim to keep the tree
      height as short as possible.
    </p>
    <p>
      On every node insertion, or deletion, we need to ensure we have not
      violated the red-black properties. If we do, then there are two possible
      steps that we have to consider in order to keep the tree appropriately
      balanced (which we‚Äôll check in this order):
    </p>
    <ol type="1">
      <li>
        <strong>Recolour the node</strong> in the case of a red node no longer
        having two black child nodes.
      </li>
      <li>
        <strong>Make a</strong>
        <a href="https://en.wikipedia.org/wiki/Tree_rotation"
          ><strong>rotation</strong></a
        >
        <strong>(left/right)</strong> in the case where recolouring then
        requires a structural change.
      </li>
    </ol>
    <p>
      The goal of a rotation is to decrease the height of the tree. The way we
      do this is by moving larger subtrees up the tree, and smaller subtrees
      down the tree. We rotate in the direction of the smaller subtree, so if
      the smaller side is the right side we‚Äôll do a right rotation.
    </p>
    <blockquote>
      <p>
        Note: there is an inconsistency between what node/subtree is affected by
        a rotation. Does the subtree being moved into the parent position
        indicate the direction or does the target node affected by the newly
        moved subtree indicate the direction (I‚Äôve opted for the latter, as
        we‚Äôll see below, but be aware of this when reading research material).
      </p>
    </blockquote>
    <p>
      In essence there are three steps that need to be applied to the target
      node (<code>T</code>) being rotated, and this is the same for either a
      left rotation or a right rotation. Let‚Äôs quickly look at both of these
      rotation movements:
    </p>
    <ul>
      <li>
        <strong>Left Rotation</strong>:
        <ol type="1">
          <li>
            <code>T</code>‚Äôs right node (<code>R</code>) is unset &amp; becomes
            <code>T</code>‚Äôs parent ‚Ä†
          </li>
          <li>
            <code>R</code>‚Äôs <em>original</em> left node <code>L</code> is now
            orphaned.
          </li>
          <li><code>T</code>‚Äôs right node is now set to <code>L</code>.</li>
        </ol>
      </li>
    </ul>
    <blockquote>
      <p>
        ‚Ä† we now find <code>R</code>‚Äôs left pointer has to be set to
        <code>T</code> (in order for it to become the parent node), meaning
        <code>R</code>‚Äôs original left pointer is orphaned.
      </p>
    </blockquote>
    <ul>
      <li>
        <strong>Right Rotation</strong>:
        <ol type="1">
          <li>
            <code>T</code>‚Äôs left node (<code>L</code>) is unset &amp; becomes
            <code>T</code>‚Äôs parent ‚Ä†
          </li>
          <li>
            <code>L</code>‚Äôs <em>original</em> right node <code>R</code> is now
            orphaned.
          </li>
          <li><code>T</code>‚Äôs left node is now set to <code>R</code>.</li>
        </ol>
      </li>
    </ul>
    <blockquote>
      <p>
        ‚Ä† we now find <code>L</code>‚Äôs right pointer has to be set to
        <code>T</code> (in order for it to become the parent node), meaning
        <code>L</code>‚Äôs original right pointer is orphaned.
      </p>
    </blockquote>
    <p>Let‚Äôs now visualize the movements for both rotations:</p>
    <p><strong>Left Rotation</strong></p>
    <p>
      <img
        src="https://www.integralist.co.uk/images/red-black-tree-left-rotation.png"
      />
    </p>
    <p><strong>Right Rotation</strong></p>
    <p>
      <img
        src="https://www.integralist.co.uk/images/red-black-tree-right-rotation.png"
      />
    </p>
    <blockquote>
      <p>
        Note: rotations are confusing, so I recommend watching
        <a href="https://www.youtube.com/watch?v=95s3ndZRGbk"
          >this short video</a
        >
        for some examples and pseudo-code.
      </p>
    </blockquote>
    <h4 id="b-tree">B-tree <a id="b-tree"></a></h4>
    <p>
      A B-tree is a sorted tree that is very similar in essence to a red-black
      tree in that it is self-balancing and as such can guarantee logarithmic
      time for search/insert/delete operations.
    </p>
    <p>
      A B-tree is useful for large read/writes of data and is commonly used in
      the design of databases and file systems, but it‚Äôs important to note that
      a B-tree is <em>not</em> a binary search tree because it allows more than
      two child nodes.
    </p>
    <p>
      The reasoning for allowing multiple children for a node is to ensure the
      height of the tree is kept as small as possible. The rationale is that
      B-trees are designed for handling huge amounts of data which itself cannot
      exist in-memory, and so that data is pulled (in chunks) from external
      sources.
    </p>
    <p>
      This type of I/O is expensive and so keeping the tree ‚Äòfat‚Äô (i.e.¬†to have
      a very short height instead of lots of node subtrees creating extra
      length) helps to reduce the amount of disk access.
    </p>
    <p>
      The design of a B-tree means that all nodes allow a set range for its
      children but not all nodes will need the full range, meaning that there is
      a potential for wasted space.
    </p>
    <blockquote>
      <p>
        Note: there are also variants of the B-tree, such as B+ trees and B*
        trees (which we‚Äôll leave as a research exercise for the reader).
      </p>
    </blockquote>
    <h4 id="weight-balanced-tree">
      Weight-balanced Tree <a id="weight-balanced-tree"></a>
    </h4>
    <p>
      A weight-balanced tree is a form of binary search tree and is similar in
      spirit to a weighted graph, in that individual nodes are ‚Äòweighted‚Äô to
      indicate the more likely successful route with regards to searching for a
      particular value.
    </p>
    <p>
      The search performance is the driving motivation for using this data
      structure, and typically used for implementing sets and dynamic
      dictionaries.
    </p>
    <h4 id="binary-heap">Binary Heap <a id="binary-heap"></a></h4>
    <p>
      A binary heap tree is a binary tree, not a binary search tree, and so it‚Äôs
      not a sorted tree. It has some additional properties that we‚Äôll look at in
      a moment, but in essence the purpose of this data structure is primarily
      to be used as the underlying implementation for a
      <a href="https://en.wikipedia.org/wiki/Priority_queue">priority queue</a>.
    </p>
    <p>The additional properties associated with a binary heap are:</p>
    <ul>
      <li>
        <strong>heap property</strong>: the node value is either greater (or
        lesser depending on the direction of the heap) or equal to the value of
        its parent.
      </li>
      <li>
        <strong>shape property</strong>: if the last level of the tree is
        incomplete, the missing nodes are filled.
      </li>
    </ul>
    <p>
      The insertion and deletion operations yield a time complexity of
      <code>O(log n)</code>.
    </p>
    <p>Below are some examples of a max and min binary heap tree structure.</p>
    <p><strong>Max Heap</strong>:</p>
    <p>
      <img src="https://www.integralist.co.uk/images/binary-heap-max.png" />
    </p>
    <p><strong>Min Heap</strong>:</p>
    <p>
      <img src="https://www.integralist.co.uk/images/binary-heap-min.png" />
    </p>
    <h3 id="hash-table">Hash Table <a id="hash-table"></a></h3>
    <p>
      A hash table is a data structure which is capable of maping ‚Äòkeys‚Äô to
      ‚Äòvalues‚Äô, and you‚Äôll typically find this is abstracted and enhanced with
      additional behaviours by many high-level programming languages such that
      they behave like an ‚Äò<a
        href="https://en.wikipedia.org/wiki/Associative_array"
        >associative array</a
      >‚Äô abstract data type.
    </p>
    <p>
      In Python it‚Äôs called a ‚Äòdictionary‚Äô and has the following structure (on
      top of which are functions such as <code>del</code>, <code>get</code> and
      <code>pop</code> etc that can manipulate the underlying data):
    </p>
    <div class="sourceCode" id="cb2">
      <pre
        class="sourceCode python"
      ><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1">table <span class="op">=</span> {<span class="st">&#39;name&#39;</span>: <span class="st">&#39;foobar&#39;</span>,</a>
<a class="sourceLine" id="cb2-2" title="2">         <span class="st">&#39;number&#39;</span>: <span class="dv">123</span>}</a></code></pre>
    </div>
    <p>
      The keys for the hash table are determined by way of a
      <a href="https://en.wikipedia.org/wiki/Hash_function">hash function</a>
      but implementors need to be mindful of hash ‚Äòcollisions‚Äô which can occur
      if the hash function isn‚Äôt able to create a distinct or unique key for the
      table.
    </p>
    <p>
      The better the hash generation, the more <em>distributed</em> the keys
      will be, and thus less likely to collide. Also the size of the underlying
      array data structure needs to accommodate the type of hash function used
      for the key generation.
    </p>
    <p>
      For example, if using modular arithmetic you might find the array needs to
      be sized to a prime number.
    </p>
    <p>
      There are many techniques for resolving hashing collisions, but here are
      two that I‚Äôve encountered:
    </p>
    <ol type="1">
      <li>Separate Chaining</li>
      <li>Linear Probing</li>
    </ol>
    <h4 id="separate-chaining">
      Separate Chaining <a id="separate-chaining"></a>
    </h4>
    <p>
      With this option our keys will contain a nested data structure, and we‚Äôll
      use a technique for storing our conflicting values into this nested
      structure, allowing us to store the same hashed value key in the top level
      of the array.
    </p>
    <h4 id="linear-probing">Linear Probing <a id="linear-probing"></a></h4>
    <p>
      With this option when a collision is found, the hash table will check to
      see if the next available index is empty, and if so it‚Äôll place the data
      into that next index.
    </p>
    <p>
      The rationale behind this technique is that because the hash table keys
      are typically quite distributed (e.g.¬†they‚Äôre rarely sequential 0, 1, 2,
      3, 4), then it‚Äôs likely that you‚Äôll have many empty empty elements and you
      can use that empty space to store your colliding data.
    </p>
    <blockquote>
      <p>
        Note: Linear Probing is suggested over Separate Chaining if your data
        structure is expected to be quite large.
      </p>
    </blockquote>
    <p>
      Personally I don‚Äôt like the idea of the Linear Probing technique as it
      feels like it‚Äôll introduce more complexity and bugs. Also, there is a
      problem with this technique which is that it relies on the top level data
      structure being an array. Which is fine if the key we‚Äôre constructing is
      numerical, but if you want to have strings for your keys then that wont
      work very well and so you‚Äôll need to be clever with how you implement
      this.
    </p>
    <h3 id="graph">Graph <a id="graph"></a></h3>
    <p>
      A graph is an abstract data type intended to guide the implementation of a
      data structure following the principles of
      <a href="https://en.wikipedia.org/wiki/Graph_theory">graph theory</a>.
    </p>
    <p>The data struture itself is non-linear and it consists of:</p>
    <ul>
      <li>
        <strong>nodes</strong>: points on the graph (also known as ‚Äòvertices‚Äô).
      </li>
      <li><strong>edges</strong>: lines connecting each node.</li>
    </ul>
    <p>
      The following image demonstrates a ‚Äòdirected‚Äô graph (notice the edges have
      arrows indicating the direction and flow):<img
        src="https://www.integralist.co.uk/images/graph-directed.png"
      />
    </p>
    <blockquote>
      <p>
        Note: an ‚Äòundirected‚Äô graph simply has no arrow heads, so the flow
        between nodes can go in either direction.
      </p>
    </blockquote>
    <p>
      Some graphs are ‚Äòweighted‚Äô which means each ‚Äòedge‚Äô has a numerical
      attribute assigned to them. These weights can indicate a stronger
      preference for a particular flow of direction.
    </p>
    <p>
      Graphs are used for representing networks (both real and electronic), such
      as streets on a map or friends on Facebook.
    </p>
    <p>When it comes to searching a graph, there are two methods:</p>
    <ol type="1">
      <li>Breadth First Search: look at siblings.</li>
      <li>Depth First Search: look at children.</li>
    </ol>
    <p>
      Which approach you choose depends on the type of values you‚Äôre searching
      for. For example, relationship across fields would lend itself to BFS,
      whereas hierarchical tree searches would be better suited to DFS.
    </p>
    <h3 id="section-1-data-structures-and-algorithms">
      Section 1: Data Structures and Algorithms
    </h3>
    <p>
      <strong>Book:</strong>
      <a
        href="https://runestone.academy/runestone/books/published/pythonds/index.html"
        >Problem Solving with Algorithms and Data Structures using Python</a
      >
    </p>
    <ul>
      <li>
        For those who needs to study the fundamental data structures and
        algorithms, highly recommend to go over above textbook thoroughly first,
        and then come back to the following content, or practice on Leetcode or
        other platform
      </li>
    </ul>
    <p><strong>Basic data structures</strong>:</p>
    <ul>
      <li>Array</li>
      <li>Linked List</li>
      <li>Stack</li>
      <li>Queue</li>
      <li>Hash Table</li>
      <li>Tree</li>
      <li>Graph</li>
    </ul>
    <p><strong>Common Algorithm Types</strong>:</p>
    <ul>
      <li>Brute Force</li>
      <li>Search and Sort</li>
      <li>Recursive</li>
      <li>Backtracking</li>
      <li>Dynamic Programming</li>
      <li>Divide and Conquer</li>
      <li>Greedy</li>
      <li>Branch and Bound</li>
    </ul>
    <p><strong>Big O Notations</strong>:</p>
    <ul>
      <li>
        It is critical that you understand and are able to calculate the Big O
        for the code you wrote.
      </li>
      <li>
        <strong
          >The order of magnitude function describes the part of T(n) that
          increases the fastest as the value of n increases. Order of magnitude
          is often called Big-O notation (for ‚Äúorder‚Äù) and written as
          O(f(n)).</strong
        >
      </li>
      <li>
        <p>Basically, the Big O measures the number of assignment statements</p>
        <table>
          <thead>
            <tr class="header">
              <th style="text-align: left">f(n)</th>
              <th style="text-align: left">Name</th>
            </tr>
          </thead>
          <tbody>
            <tr class="odd">
              <td style="text-align: left">1</td>
              <td style="text-align: left">Constant</td>
            </tr>
            <tr class="even">
              <td style="text-align: left">log n</td>
              <td style="text-align: left">Logarithmic</td>
            </tr>
            <tr class="odd">
              <td style="text-align: left">n</td>
              <td style="text-align: left">Linear</td>
            </tr>
            <tr class="even">
              <td style="text-align: left">n log n</td>
              <td style="text-align: left">Log Linear</td>
            </tr>
            <tr class="odd">
              <td style="text-align: left">n^2</td>
              <td style="text-align: left">Quadratic</td>
            </tr>
            <tr class="even">
              <td style="text-align: left">n^3</td>
              <td style="text-align: left">Cubic</td>
            </tr>
            <tr class="odd">
              <td style="text-align: left">2^n</td>
              <td style="text-align: left">Exponential</td>
            </tr>
          </tbody>
        </table>
        <p>
          <a
            href="https://camo.githubusercontent.com/0c10cd72cb4a5c73e7139648bcb79b107fb6df017d84b8f321fea2aa63d4d47d/68747470733a2f2f72756e6573746f6e652e61636164656d792f72756e6573746f6e652f626f6f6b732f7075626c69736865642f707974686f6e64732f5f696d616765732f6e6577706c6f742e706e67"
            ><img
              src="https://camo.githubusercontent.com/0c10cd72cb4a5c73e7139648bcb79b107fb6df017d84b8f321fea2aa63d4d47d/68747470733a2f2f72756e6573746f6e652e61636164656d792f72756e6573746f6e652f626f6f6b732f7075626c69736865642f707974686f6e64732f5f696d616765732f6e6577706c6f742e706e67"
              alt="BigO Image"
          /></a>
        </p>
      </li>
    </ul>
    <h4 id="chapter-1-data-structures">Chapter 1: Data Structures</h4>
    <p><strong>1.1 Array</strong></p>
    <ul>
      <li>
        An array (in Python its called <em>list</em>) is a collection of items
        where each item holds a relative position with respect to the others.
      </li>
    </ul>
    <p><strong>1.2 Linked List</strong></p>
    <ul>
      <li>
        Similar to array, but requires O(N) time on average to visit an element
        by index
      </li>
      <li>
        Linked list utilize memory better than array, since it can use discrete
        memory space, whereas array must use continuous memory space
      </li>
      <li>
        <a
          href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/linked_list.md"
          >Details and Templates</a
        >
      </li>
    </ul>
    <p><strong>1.3 Stack</strong></p>
    <ul>
      <li>
        Stacks are fundamentally important, as they can be used to reverse the
        order of items.
      </li>
      <li>The order of insertion is the reverse of the order of removal.</li>
      <li>Stack maintain a FILO (first in last out) ordering property.</li>
      <li>
        When pop is called on the end of the list it takes O(1) but when pop is
        called on the first element in the list or anywhere in the middle it is
        O(n) (in Python).
      </li>
    </ul>
    <p><strong>1.3.1 Arithmetic Expressions</strong></p>
    <ul>
      <li>
        Infix: the operator is in between the two operands that it is working on
        (i.e.¬†A+B)
        <ul>
          <li>
            Fully Parenthesized expression: uses one pair of parentheses for
            each operator. (i.e.¬†((A + (B * C)) + D))
          </li>
        </ul>
      </li>
      <li>
        Prefix: all operators precede the two operands that they work on
        (i.e.¬†+AB)
      </li>
      <li>
        Postfix: operators come after the corresponding operands (i.e.¬†AB+)
      </li>
    </ul>
    <table>
      <thead>
        <tr class="header">
          <th style="text-align: left">Infix Expression</th>
          <th style="text-align: left">Prefix Expression</th>
          <th style="text-align: left">Postfix Expression</th>
        </tr>
      </thead>
      <tbody>
        <tr class="odd">
          <td style="text-align: left">A + B</td>
          <td style="text-align: left">+ A B</td>
          <td style="text-align: left">A B +</td>
        </tr>
        <tr class="even">
          <td style="text-align: left">A + B * C</td>
          <td style="text-align: left">+ A * B C</td>
          <td style="text-align: left">A B C * +</td>
        </tr>
        <tr class="odd">
          <td style="text-align: left">(A + B) * C</td>
          <td style="text-align: left">* + A B C</td>
          <td style="text-align: left">A B + C *</td>
        </tr>
        <tr class="even">
          <td style="text-align: left">A + B * C + D</td>
          <td style="text-align: left">+ + A * B C D</td>
          <td style="text-align: left">A B C * + D +</td>
        </tr>
        <tr class="odd">
          <td style="text-align: left">(A + B) * (C + D)</td>
          <td style="text-align: left">* + A B + C D</td>
          <td style="text-align: left">A B + C D + *</td>
        </tr>
        <tr class="even">
          <td style="text-align: left">A * B + C * D</td>
          <td style="text-align: left">+ * A B * C D</td>
          <td style="text-align: left">A B * C D * +</td>
        </tr>
        <tr class="odd">
          <td style="text-align: left">A + B + C + D</td>
          <td style="text-align: left">+ + + A B C D</td>
          <td style="text-align: left">A B + C + D +</td>
        </tr>
      </tbody>
    </table>
    <ul>
      <li>
        <strong>NOTE:</strong>
        <ul>
          <li>
            Only infix notation requires parentheses to determine precedence
          </li>
          <li>
            The order of operations within prefix and postfix expressions is
            completely determined by the position of the operator and nothing
            else
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>1.4 Queue</strong></p>
    <ul>
      <li>
        A queue is structured as an ordered collection of items which are added
        at one end, called the ‚Äúrear,‚Äù and removed from the other end, called
        the ‚Äúfront.‚Äù
      </li>
      <li>Queues maintain a FIFO ordering property.</li>
      <li>
        A <em><strong>deque</strong></em
        >, also known as a double-ended queue, is an ordered collection of items
        similar to the queue.
        <ul>
          <li>
            It has two ends, a front and a rear, and the items remain positioned
            in the collection.
          </li>
          <li>New items can be added at either the front or the rear.</li>
          <li>Likewise, existing items can be removed from either end.</li>
        </ul>
      </li>
    </ul>
    <p><strong>1.5 Hash Table</strong></p>
    <ul>
      <li>
        A <strong>hash table</strong> is a collection of items which are stored
        in such a way as to make it easy to find them later.
      </li>
      <li>
        Each position of the hash table, often called a slot, can hold an item
        and is named by an integer value starting at 0.
      </li>
      <li>
        The mapping between an item and the slot where that item belongs in the
        hash table is called the <strong>hash function</strong>.
        <ul>
          <li>
            <strong>Remainder method</strong> takes an item and divides it by
            the table size, returning the remainder as its hash value
            (i.e.¬†<code>h(item) = item % 11</code>)
          </li>
          <li>
            <strong>load factor</strong> is the number of items devided by the
            table size
          </li>
          <li>
            <strong>collision</strong> refers to the situation that multiple
            items have the same hash value
          </li>
          <li>
            <strong>folding method</strong> for constructing hash functions
            begins by dividing the item into equal-size pieces (the last piece
            may not be of equal size). These pieces are then added together to
            give the resulting hash value.
          </li>
          <li>
            <strong>mid-square method</strong> first squares the item, and then
            extract some portion of the resulting digits. For example, 44^2 =
            1936, extract middle two digits 93, then perform remainder step
            (93%11=5).
          </li>
        </ul>
      </li>
      <li>
        <strong>Collision Resolution</strong> is the process to systemacticly
        place the second item in the hash table when two items hash to the same
        slot.
      </li>
      <li>
        <strong>Open addressing (linear probing):</strong> sequentially find the
        next open slot or address in the hash table
        <ul>
          <li>
            A disadvantage to linear probing is the tendency for clustering;
            items become clustered in the table.
          </li>
          <li>
            <strong>Rehashing</strong> is one way to deal with clustering, which
            is to skip the slot when looking sequentially for the next open
            slot, thereby more evenly distributing the items that have caused
            collisions.
          </li>
        </ul>
      </li>
      <li>
        <strong>Quadratic probing:</strong> instead of using a constant ‚Äúskip‚Äù
        value, we use a rehash function that increments the hash value by 1, 3,
        5, 7, 9, and so on. This means that if the first hash value is h, the
        successive values are h+1, h+4, h+9, h+16, and so on.
      </li>
      <li>
        <strong>Chaining</strong> allows many items to exist at the same
        location in the hash table.
        <ul>
          <li>
            When collisions happen, the item is still placed in the proper slot
            of the hash table.
          </li>
          <li>
            As more and more items hash to the same location, the difficulty of
            searching for the item in the collection increases.
            <a
              href="https://camo.githubusercontent.com/2f688991ef488f29242d72b62e9f5137545ee3653290adb7557dd875e5851c28/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f636861696e696e672e706e67"
              ><img
                src="https://camo.githubusercontent.com/2f688991ef488f29242d72b62e9f5137545ee3653290adb7557dd875e5851c28/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f636861696e696e672e706e67"
            /></a>
          </li>
        </ul>
      </li>
      <li>
        The initial size for the hash table has to be a prime number so that the
        collision resolution algorithm can be as efficient as possible.
      </li>
    </ul>
    <p><strong>1.6 Trees</strong></p>
    <ul>
      <li>
        A tree data structure has its root at the top and its leaves on the
        bottom.
      </li>
      <li>
        Three properties of tree:
        <ol type="1">
          <li>
            we start at the top of the tree and follow a path made of circles
            and arrows all the way to the bottom.
          </li>
          <li>
            all of the children of one node are independent of the children of
            another node.
          </li>
          <li>each leaf node is unique.</li>
        </ol>
      </li>
      <li>
        <strong>binary tree:</strong> each node in the tree has a maximum of two
        children.
        <ul>
          <li>
            A <strong>balanced binary tree</strong> has roughly the same number
            of nodes in the left and right subtrees of the root.
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>1.6.1 Tree Traversal: access the nodes of the tree</strong></p>
    <ul>
      <li>Tree traversal is the foundation of all tree related problems.</li>
      <li>
        Here are a few different ways to traverse a tree:
        <ul>
          <li>BFS: Level-order</li>
          <li>DFS: Pre-order, In-order, Post-order</li>
          <li>
            <a
              href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/tree_traversal.md"
              >Details and Templates</a
            >
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>1.6.2 Binary Search Tree (BST)</strong></p>
    <ul>
      <li>
        BST Property (left subtree &lt; root &lt; right subtree):
        <ol type="1">
          <li>
            The value in each node must be
            <code>greater than (or equal to)</code> any values stored in its
            left subtree.
          </li>
          <li>
            The value in each node must be
            <code>less than (or equal to)</code> any values stored in its right
            subtree.
          </li>
        </ol>
      </li>
      <li>
        <code>Inorder traversal</code> in BST will be in
        <code>ascending order</code>. Therefore, the inorder traversal is the
        most frequent used traversal method of a BST.
      </li>
      <li>
        <strong>successor:</strong> the node that has the next-largest key in
        the tree
        <ul>
          <li>it has no more than one child</li>
        </ul>
      </li>
      <li>
        You could go over the
        <a
          href="https://leetcode.com/explore/learn/card/introduction-to-data-structure-binary-search-tree/"
          >Leetcode Binary Search Tree topic</a
        >
        for details
      </li>
    </ul>
    <p><strong>1.6.3 Heap / Priority Queue / Binary Heap</strong></p>
    <ul>
      <li>
        <strong>Priority Queue:</strong>
        <ul>
          <li>
            the logical order of items inside a queue is determined by their
            priority.
          </li>
          <li>
            The highest priority items are at the front of the queue and the
            lowest priority items are at the back.
          </li>
        </ul>
      </li>
      <li>
        <strong>Binary Heap:</strong> the classic way to implement a priority
        queue.
        <ul>
          <li>both enqueue and dequeue items are <strong>O(logn)</strong></li>
          <li>
            <strong>min heap:</strong> the smallest key is always at the front
          </li>
          <li>
            <strong>max heap:</strong> the largest key value is always at the
            front
          </li>
          <li>
            <strong>complete binary tree:</strong> a tree in which each level
            has all of its nodes (except the bottom level)
            <ul>
              <li>can be implemented using a single list</li>
              <li>
                Because the tree is complete, the left child of a parent (at
                position <strong>p</strong>) is the node that is found in
                position <strong>2p</strong> in the list. Similarly, the right
                child of the parent is at position <strong>2p+1</strong> in the
                list.
                <a
                  href="https://camo.githubusercontent.com/8d52ed9541e243376b0824903076f293cee0a483c1487ed9d204e29637983c9a/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f686561704f726465722e706e67"
                  ><img
                    src="https://camo.githubusercontent.com/8d52ed9541e243376b0824903076f293cee0a483c1487ed9d204e29637983c9a/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f686561704f726465722e706e67"
                /></a>
              </li>
            </ul>
          </li>
          <li>
            <strong>heap order property:</strong> In a heap, for every node
            <strong>x</strong> with parent <strong>p</strong>, the key in
            <strong>p</strong> is smaller than or equal to the key in
            <strong>x</strong>.
            <ul>
              <li>
                For example, the root of the tree must be the smallest item in
                the tree
              </li>
            </ul>
          </li>
          <li>
            When to use heap:
            <ul>
              <li>Priority Queue implementation</li>
              <li>
                whenever need quick access to largest/smallest item
                <ul>
                  <li>Instant access to the item</li>
                  <li>insertions are fast, allow in-place sorting</li>
                </ul>
              </li>
              <li>
                More details can be seen in
                <a
                  href="https://stackoverflow.com/questions/749199/when-would-i-want-to-use-a-heap"
                  >this discussion</a
                >
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>1.6.4 More Trees</strong></p>
    <ul>
      <li>
        <em><strong>Parse tree</strong></em> can be used to represent real-world
        constructions like sentences or mathematical expressions.
        <ul>
          <li>
            A simple solution to keeping track of parents as we traverse the
            tree is to use a stack.
          </li>
          <li>
            When we want to descend to a child of the current node, we first
            push the current node on the stack.
          </li>
          <li>
            When we want to return to the parent of the current node, we pop the
            parent off the stack.
          </li>
        </ul>
      </li>
      <li>
        <em><strong>AVL Tree</strong></em
        >: a balanced binary tree. the AVL is named for its inventors G.M.
        Adelson-Velskii and E.M. Landis.
        <ul>
          <li>
            For each node: <em>balanceFactor</em> =
            <em>height(leftSubTree)</em> ‚àí <em>height(rightSubTree)</em>
          </li>
          <li>a subtree is left-heavy if <em>balance_factor &gt; 0</em></li>
          <li>a subtree is right-heavy if <em>balance_factor &lt; 0</em></li>
          <li>
            a subtree is perfectly in balance if <em>balance_factor = 0</em>
          </li>
          <li>
            For simplicity we can define a tree to be in balance if the balance
            factor is -1, 0, or 1.
          </li>
          <li>
            The number of nodes follows the pattern of
            <em>Fibonacci sequence</em>, as the number of elements get larger
            the ratio of Fi/Fi-1 closes to the golden ratio, so the time
            complexity is derived to be <strong>O(log n)</strong>
          </li>
        </ul>
      </li>
      <li>
        <em><strong>Red-Black Tree</strong></em>
        <ul>
          <li>
            <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree"
              >Details in Wiki</a
            >
          </li>
        </ul>
      </li>
      <li>
        <em><strong>B+ Tree</strong></em
        >: N-array tree
        <ul>
          <li>
            <a href="https://en.wikipedia.org/wiki/B%2B_tree"
              >Details in Wiki</a
            >
          </li>
        </ul>
      </li>
      <li>
        <em><strong>Trie</strong></em>
        <ul>
          <li><em>This is a common data structure in interviews</em></li>
          <li>
            <a
              href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/trie.md"
              >Template</a
            >
          </li>
        </ul>
      </li>
      <li>
        <em><strong>Binary Index Tree (Fenwick Tree)</strong></em>
        <ul>
          <li>
            <a
              href="https://www.geeksforgeeks.org/binary-indexed-tree-or-fenwick-tree-2/"
              >Binary Index Tree (Fenwick Tree)</a
            >
          </li>
          <li>
            <a
              href="https://leetcode.com/problems/count-of-smaller-numbers-after-self/"
              >315. Count of Smaller Numbers After Self</a
            >
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>1.7 Graph</strong></p>
    <p><strong>1.7.1 Vocabulary and Definitions</strong></p>
    <ul>
      <li>
        <strong>Vertex (or Node):</strong> the name is called ‚Äúkey‚Äù and the
        additional information is called ‚Äúpayload‚Äù
      </li>
      <li>
        <strong>Edge (or arc):</strong> it connects two vertices to show that
        there is a relationship between them.
        <ul>
          <li>
            One way edge is called <strong>directed graph (or digraph)</strong>
          </li>
        </ul>
      </li>
      <li>
        <strong>Weight:</strong> edges maybe weighted to show that there is a
        coset to fo from one vertex to another.
      </li>
      <li>
        <strong>Path:</strong> a sequence of vertices that are connected bny
        edges
        <ul>
          <li>
            Unweighted path length is the number of edges in the path,
            specifically n-
          </li>
          <li>
            Weighted path is the sum of the weights of all the edges in the path
          </li>
        </ul>
      </li>
      <li>
        <strong>Cycle:</strong> a path that starts and ends at the same vertex
        <ul>
          <li>
            A graph with no cycles is called an <strong>acyclic graph</strong>.
          </li>
          <li>
            A directed graph with no cycles is called a
            <strong>directed acyclic graph (or DAG)</strong>
          </li>
        </ul>
      </li>
      <li>
        <strong>Graph:</strong> a graph (G) is composed with a set of vertices
        (V) and edges (E) Each edge is a tuple of vertex and weight (v,w).
        G=(V,E) where w,v‚ààV
      </li>
    </ul>
    <p><strong>1.7.2 Graph Representation</strong></p>
    <ul>
      <li>
        Adjacency Matrix (2D matrix)
        <ul>
          <li>Good when number of edges is large</li>
          <li>Each of the rows and columns represent a vertex in the graph.</li>
          <li>
            The value in the cell at the intersection of row v and column w
            indicates if there is an edge from vertex v to vertex w. It also
            represents the weight of the edge from vertex v to vertex w.
          </li>
          <li>
            When two vertices are connected by an edge, we say that they are
            <strong>adjacent</strong>
            <a
              href="https://camo.githubusercontent.com/72d272bc0fbbee67b8b6a54f62208602d612edc61052a55c7fa339bd955ff4df/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f61646a4d61742e706e67"
              ><img
                src="https://camo.githubusercontent.com/72d272bc0fbbee67b8b6a54f62208602d612edc61052a55c7fa339bd955ff4df/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f61646a4d61742e706e67"
            /></a>
          </li>
          <li>
            <strong>sparse:</strong> most of the cells in the matrix are empty
          </li>
        </ul>
      </li>
      <li>
        Adjacency List
        <ul>
          <li>space-efficient way for implementation</li>
          <li>
            keep a master list of all the vertices in the Graph object. each
            vertex is an element of the list with the vertex as ID and a list of
            its adjacent vertices as value
            <a
              href="https://camo.githubusercontent.com/57590f7986a5ec8f3e66b93edd06da65ed00c57b199a11d210a4aa10e19cf6ce/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f61646a6c6973742e706e67"
              ><img
                src="https://camo.githubusercontent.com/57590f7986a5ec8f3e66b93edd06da65ed00c57b199a11d210a4aa10e19cf6ce/687474703a2f2f696e746572616374697665707974686f6e2e6f72672f72756e6573746f6e652f7374617469632f707974686f6e64732f5f696d616765732f61646a6c6973742e706e67"
            /></a>
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>1.7.3 Graph Algorithms</strong></p>
    <ul>
      <li>
        Graph traversal: BFS &amp; DFS
        <ul>
          <li>
            <a
              href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/graph_traversal.md"
              >Template</a
            >
          </li>
        </ul>
      </li>
      <li>
        Graph Algorithms:
        <ul>
          <li>
            Shortest Path:
            <ul>
              <li>
                Dijkstra‚Äôs Algorithm (Single source point)
                <ul>
                  <li>
                    <em
                      ><strong
                        >Essentially, this is a BFS using priority queue instead
                        of queue</strong
                      ></em
                    >
                  </li>
                  <li>
                    <a
                      href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/dijkstra.md"
                      >Template</a
                    >
                  </li>
                </ul>
              </li>
              <li>Floyd Warshall Algorithm (Multiple source point)</li>
            </ul>
          </li>
          <li>
            Topological Sort
            <ul>
              <li>
                <a
                  href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/topological_sort.md"
                  >Template</a
                >
              </li>
            </ul>
          </li>
          <li>
            Strongly Connected Components
            <ul>
              <li>
                <a
                  href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/graph_SCC.md"
                  >More Info</a
                >
              </li>
            </ul>
          </li>
          <li>
            Prim‚Äôs Spanning Tree Algorithm
            <ul>
              <li>
                <a
                  href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/prim_spanning_tree.md"
                  >More Info</a
                >
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
    <h4 id="chapter-2-common-algorithm-types">
      Chapter 2: Common Algorithm Types
    </h4>
    <p><strong>2.1 Brute Force</strong></p>
    <ul>
      <li>Most common algorithm</li>
      <li>
        Whenever you are facing a problem without many clues, you should solve
        it using brute force first, and observe the process and try to optimize
        your solution
      </li>
    </ul>
    <p><strong>2.2 Search</strong></p>
    <p><strong>2.2.1 Sequential Search</strong></p>
    <ul>
      <li>
        Sequential Search: visit the stored value in a sequence (use loop)
      </li>
    </ul>
    <p><strong>2.2.2 Binary Search</strong></p>
    <ul>
      <li>Examine the middle item of an ordered list</li>
      <li>KEY is the search interval</li>
      <li>
        <a
          href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/binary_search.md"
          >Template</a
        >
      </li>
    </ul>
    <p><strong>2.3 Sort</strong></p>
    <p><strong>2.3.1 Bubble Sort</strong></p>
    <ul>
      <li>
        Compares adjacent items and exchanges those that are out of order.
      </li>
      <li>
        <strong>Short bubble:</strong> stop early if it finds that the list has
        become sorted.
      </li>
      <li>time complexity: O(n2)</li>
    </ul>
    <p><strong>2.3.2 Selection Sort</strong></p>
    <ul>
      <li>
        Looks for the largest value as it makes a pass and, after completing the
        pass, places it in the proper location.
      </li>
      <li>time complexity: O(n2)</li>
    </ul>
    <p><strong>2.3.3 Insertion Sort</strong></p>
    <ul>
      <li>Maintains a sorted sub-list in the lower positions of the list.</li>
      <li>
        Each new item is then ‚Äúinserted‚Äù back into the previous sub-list such
        that the sorted sub-list is one item larger.
      </li>
      <li>time complexity: O(n2)</li>
    </ul>
    <p><strong>2.3.4 Shell Sort</strong></p>
    <ul>
      <li>
        Breaks the original list into a number of smaller sub-lists, each of
        which is sorted using an insertion sort.
        <ul>
          <li>
            the shell sort uses an increment <em>i</em>, sometimes called the
            <strong>gap</strong>, to create a sub-list by choosing all items
            that are <em>i</em> items apart.
          </li>
          <li>
            After all the sub-lists are sorted, it finally does a standard
            insertion sort
          </li>
          <li>
            time complexity goes between O(n) and O(n2), by changing the
            increment, a shell sort can perform at O(n^(3/2)).
          </li>
        </ul>
      </li>
    </ul>
    <p><strong>2.3.5 Merge Sort</strong></p>
    <ul>
      <li>A recursive algorithm that continually splits a list in half.</li>
      <li>
        <a
          href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/merge_sort.md"
          >Details and Templates</a
        >
      </li>
    </ul>
    <p><strong>2.3.6 Quick Sort</strong></p>
    <ul>
      <li>
        First selects a value (<strong>pivot value</strong>), and then use this
        value to assist with splitting the list.
      </li>
      <li>
        <a
          href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/quick_sort.md"
          >Details and Templates</a
        >
      </li>
    </ul>
    <p><strong>2.3.7 Heap Sort</strong></p>
    <ul>
      <li>Use the property of heap to sort the list</li>
    </ul>
    <p><strong>2.4 Recursion</strong></p>
    <p>
      <strong>Recursion</strong> is a method of solving problems that involves
      breaking a problem down into smaller and smaller sub-problems until you
      get to a small enough problem that it can be solved trivially. Usually
      recursion involves a function calling itself.
    </p>
    <p>Three Laws of Recursion:</p>
    <ol type="1">
      <li>A recursive algorithm must have a base case.</li>
      <li>
        A recursive algorithm must change its state and move toward the base
        case.
      </li>
      <li>A recursive algorithm must call itself, recursively.</li>
    </ol>
    <p>Recursive visualization: Fractal tree</p>
    <ul>
      <li>
        A <strong>fractal</strong> is something that looks the same at all
        different levels of magnification.
      </li>
      <li>
        A fractal tree: a small twig has the same shape and characteristics as a
        whole tree.
      </li>
    </ul>
    <p><strong>2.4.1 Recursive function in Python</strong></p>
    <ul>
      <li>
        When a function is called in Python, a stack frame is allocated to
        handle the local variables of the function.
      </li>
      <li>
        When the function returns, the return value is left on top of the stack
        for the calling function to access.
      </li>
      <li>
        Even though we are calling the same function over and over, each call
        creates a new scope for the variables that are local to the function.
      </li>
    </ul>
    <p><strong>2.5 Backtracking</strong></p>
    <ul>
      <li>
        a general algorithm for finding all (or some) solutions to constraint
        satisfaction problems (i.e.¬†chess, puzzles, crosswords, verbal
        arithmetic, Sudoku, etc)
      </li>
      <li>
        <a
          href="https://github.com/zmcddn/coding-interview-guide/blob/master/Templates/backtrack.md"
          >Template</a
        >
      </li>
    </ul>
    <p><strong>2.6 Dynamic Programming</strong></p>
    <p>
      <strong>Dynamic Programming (DP)</strong> is an algorithm technique which
      is usually based on a recurrent formula and one (or some) starting states.
      - A sub-solution of the problem is constructed from previously found ones.
      - Usually used to find the extreme cases such as shortest path, best fit,
      smallest set, etc.
    </p>
    <p><strong>2.7 Divide and Conquer</strong></p>
    <ul>
      <li>
        <strong>Divide</strong>: break into non-overlapping sub-problems of the
        same type (of problem)
      </li>
      <li><strong>Conquer</strong>: solve sub-problems</li>
      <li>
        the algorithm is to keep dividing and conquering, and finally combine
        them to get the solution
      </li>
      <li>the algorithm can be written in recursive or loop</li>
    </ul>
    <p><strong>2.8 Greedy</strong></p>
    <p><strong>Greedy algorithm:</strong></p>
    <ul>
      <li>find a safe move first</li>
      <li>prove safety</li>
      <li>solve subproblem (which should be similar to original problem)</li>
      <li>estimate running time</li>
    </ul>
  </body>
</html>
