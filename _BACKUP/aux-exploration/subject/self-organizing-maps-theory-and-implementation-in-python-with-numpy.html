<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>self-organizing-maps-theory-and-implementation-in-python-with-numpy</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
</head>
<body>
<h1 id="self-organizing-maps-theory-and-implementation-in-python-with-numpy">Self-Organizing Maps: Theory and Implementation in Python with NumPy</h1>
<h4 id="introduction">Introduction <a id="introduction"></a></h4>
<p>In this guide, we’ll be taking a look at an unsupervised learning model, known as a <em><strong>Self-Organizing Map (SOM)</strong></em>, as well as its implementation in Python. We’ll be using an <strong>RGB Color</strong> example to train the SOM and demonstrate its performance and typical usage.</p>
<h4 id="self-organizing-maps-a-general-introduction">Self-Organizing Maps: A General Introduction <a id="selforganizingmapsageneralintroduction"></a></h4>
<p>A <em>Self-Organizing Map</em> was first introduced by <a href="https://link.springer.com/article/10.1007/BF00337288">Teuvo Kohonen in 1982</a> and is also sometimes known as a <em><strong>Kohonen map</strong></em>. It is a special type of an <em>artificial neural network</em>, which builds a map of the training data. The map is generally a 2D rectangular grid of weights but can be extended to a 3D or higher dimensional model. Other grid structures like hexagonal grids are also possible.</p>
<p>An SOM is mainly used for data visualization and provides a quick visual summary of the training instances. In a 2D rectangular grid, each cell is represented by a weight vector. For a trained SOM, each cell weight represents a summary of a few training examples. Cells in the close vicinity of each other have similar weights, and like examples can be mapped to cells in a small neighborhood of each other.</p>
<p>The figure below is a rough illustration of the structure of the SOM:</p>
<figure>
<img src="https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-0.png" alt="self-organizing map illustration" /><figcaption>self-organizing map illustration</figcaption>
</figure>
<p>An SOM is trained using <strong>competitive learning</strong>.</p>
<blockquote>
<p><strong>Competitive Learning</strong> is a form of unsupervised learning, where constituent elements compete to produce a satisfying result, and only one gets to win the competition.</p>
</blockquote>
<p>When a training example is input into the grid, the <em><strong>Best Matching Unit (BMU)</strong></em> is determined (competition winner). The BMU is the cell whose weights are closest to the training example.</p>
<p>Next, the BMU’s weights and weights of the cells neighboring the BMU, are adapted to move closer to the input training instance. While there are other valid variants of training an SOM, we present the most popular and widely used implementation of the SOM in this guide.</p>
<p>As we’ll be using some Python routines to demonstrate the functions used to train an SOM, let’s import a few of the libraries we’ll be using:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb1-1" title="1"><span class="im">import</span> numpy <span class="im">as</span> np</a>
<a class="sourceLine" id="cb1-2" title="2"><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</a></code></pre></div>
<h4 id="the-algorithm-behind-training-self-organizing-maps">The Algorithm Behind Training Self-Organizing Maps <a id="thealgorithmbehindtrainingselforganizingmaps"></a></h4>
<p>The basic algorithm for training an SOM is given below:</p>
<ul>
<li>Initialize all grid weights of the SOM</li>
<li>Repeat until convergence or maximum epochs are reached
<ul>
<li>Shuffle the training examples</li>
<li>For each training instance xx
<ul>
<li>Find the best matching unit BMU</li>
<li>Update the weight vector of BMU and its neighboring cells</li>
</ul></li>
</ul></li>
</ul>
<p>The three steps for initialization, finding the BMU, and updating the weights are explained in the following sections. Let’s begin!</p>
<p><strong>Initializing the SOM GRID</strong></p>
<p>All the SOM grid weights can be initialized randomly. The SOM grid weights can also be initialized by randomly chosen examples from the training dataset.</p>
<blockquote>
<p>Which one should you choose?</p>
</blockquote>
<p>SOMs are sensitive to the initial weight of the map, so this choice affects the overall model. According to <a href="https://arxiv.org/ftp/arxiv/papers/1210/1210.5873.pdf">a case study</a> performed by Ayodeji and Evgeny of University of Leicester and Siberian Federal University:</p>
<blockquote>
<p>By comparing the proportion of final SOM map of <strong>RI (Random Initialization)</strong> which outperformed <strong>PCI (Principal Component Initialization)</strong> under the same conditions, it was observed that RI performed quite well for non-linear data sets.</p>
<p>However for quasi-linear datasets, the result remains inconclusive. In general, we can conclude that the hypothesis about advantages of the PCI is definitely wrong for essentially nonlinear datasets.</p>
</blockquote>
<p>Random initialization outperforms non-random initialization for non-linear datasets. For quasi-linear datasets, it’s not quite clear what approach wins consistently. Given these results - we’ll stick to <strong>random initialization</strong>.</p>
<p><strong>Finding the Best Matching Unit (BMU)</strong></p>
<p>As mentioned earlier, the best matching unit is the cell of the SOM grid that is closest to the training example xx. One method of finding this unit is to compute the <strong>Euclidean distance</strong> of xx from the weight of each cell of the grid.</p>
<blockquote>
<p>The cell with the minimum distance can be chosen as the BMU.</p>
</blockquote>
<p>An important point to note is that Euclidean distance is not the only possible method of selecting the BMU. An alternative distance measure or a similarity metric can also be used to determine the BMU, and choosing this mainly depends on the data and model you’re building specifically.</p>
<p><strong>Updating the Weight Vector of BMU and Neighboring Cells</strong></p>
<p>A training example xx effects various cells of the SOM grid by pulling the weights of these cells towards it. The maximum change occurs in the BMU and the influence of xx diminishes as we move away from the BMU in the SOM grid. For a cell with coordinates (i,j)(i,j), its weight wijwij is updated at epoch t+1t+1 as:</p>
<p>w(t+1)ij←w(t)ij+Δw(t)ijwij(t+1)←wij(t)+Δwij(t)</p>
<p>Where Δw(t)ijΔwij(t) is the change to be added to w(t)ijwij(t). It can be computed as:</p>
<p>Δw(t)ij=η(t)fi,j(g,h,σt)(x−w(t)ij)Δwij(t)=η(t)fi,j(g,h,σt)(x−wij(t))</p>
<p>For this expression:</p>
<ul>
<li>tt is the epoch number</li>
<li>(g,h)(g,h) are the coordinates of BMU</li>
<li>ηη is the learning rate</li>
<li>σtσt is the radius</li>
<li>fij(g,h,σt)fij(g,h,σt) is the neighborhood distance function</li>
</ul>
<p>In the following sections, we’ll present the details of this weight training expression.</p>
<p><strong>The Learning Rate</strong></p>
<p>The learning rate ηη is a constant in the range [0,1] and determines the step size of the weight vector towards the input training example. For η=0η=0, there is no change in the weight, and when η=1η=1 the weight vector wijwij take the value of xx.</p>
<p>ηη is kept high at the start and decayed as the epochs proceed. One strategy for reducing the learning rate during the training phase is to use exponential decay:</p>
<p>η(t)=η0e−t∗λη(t)=η0e−t∗λ</p>
<p>Where λ&lt;0λ&lt;0 is the decay rate.</p>
<p>To understand how the learning rate changes with the decay rate, let’s plot the learning rate against various epochs when the initial learning rate is set to one:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb2-1" title="1">epochs <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb2-2" title="2">lr_decay <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.1</span>, <span class="fl">0.5</span>, <span class="fl">0.99</span>]</a>
<a class="sourceLine" id="cb2-3" title="3">fig,ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb2-4" title="4">plt_ind <span class="op">=</span> np.arange(<span class="dv">4</span>) <span class="op">+</span> <span class="dv">141</span></a>
<a class="sourceLine" id="cb2-5" title="5"><span class="cf">for</span> decay, ind <span class="kw">in</span> <span class="bu">zip</span>(lr_decay, plt_ind):</a>
<a class="sourceLine" id="cb2-6" title="6">    plt.subplot(ind)</a>
<a class="sourceLine" id="cb2-7" title="7">    learn_rate <span class="op">=</span> np.exp(<span class="op">-</span>epochs <span class="op">*</span> decay)</a>
<a class="sourceLine" id="cb2-8" title="8">    plt.plot(epochs, learn_rate, c<span class="op">=</span><span class="st">&#39;cyan&#39;</span>)</a>
<a class="sourceLine" id="cb2-9" title="9">    plt.title(<span class="st">&#39;decay rate: &#39;</span> <span class="op">+</span> <span class="bu">str</span>(decay))</a>
<a class="sourceLine" id="cb2-10" title="10">    plt.xlabel(<span class="st">&#39;epochs $t$&#39;</span>)</a>
<a class="sourceLine" id="cb2-11" title="11">    plt.ylabel(<span class="st">&#39;$\eta^(t)$&#39;</span>)</a>
<a class="sourceLine" id="cb2-12" title="12">fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.5</span>, wspace<span class="op">=</span><span class="fl">0.3</span>)</a>
<a class="sourceLine" id="cb2-13" title="13">plt.show()</a></code></pre></div>
<figure>
<img src="https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-1.png" alt="learning rates for self organizing maps" /><figcaption>learning rates for self organizing maps</figcaption>
</figure>
<p><strong>The Neighborhood Distance Function</strong></p>
<p>The neighborhood distance function is given by:</p>
<p>fij(g,h,σt)=e−d((i,j),(g,h))22σ2tfij(g,h,σt)=e−d((i,j),(g,h))22σt2</p>
<p>where d((i,j),(g,h))d((i,j),(g,h)) is the distance of coordinates (i,j)(i,j) of a cell from the BMU’s coordinates (g,h)(g,h), and σtσt is the radius at epoch tt. Normally Euclidean distance is used to compute the distance, however, any other distance or similarity metric can be used.</p>
<p>As the distance of BMU with itself is zero, the weight change of the BMU reduces to:</p>
<p>Δwgh=η(x−wgh)Δwgh=η(x−wgh)</p>
<p>For a unit (i,j)(i,j) having a large distance from the BMU, the neighborhood distance function reduces to a near-zero value, leading to a very small magnitude of ΔwijΔwij. Hence, such units are unaffected by the training example xx. One training example, therefore, only affects the BMU and the cells in the close vicinity of the BMU. As we move away from the BMU the change in weights becomes less and less until it is negligible.</p>
<p>The radius determines the influence region of a training example xx. A high radius value affects a larger number of cells and a smaller radius affects only the BMU. A common strategy is to start with a large radius and reduce it as the epochs proceed, i.e.:</p>
<p>σt=σ0e−t∗βσt=σ0e−t∗β</p>
<p>Here β&lt;0β&lt;0 is the decay rate. The decay rate corresponding to radius has the same effect on the radius as the decay rate corresponding to the learning rate. To gain a deeper insight into the behavior of the neighborhood function, let’s plot it against the distance for different values of the radius. A point to note in these graphs is that the distance function approaches a near-zero value as the distance exceeds 10 for σ2≤10σ2≤10.</p>
<p>We’ll use this fact later to make training more efficient in the implementation part:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">distance <span class="op">=</span> np.arange(<span class="dv">0</span>, <span class="dv">30</span>)</a>
<a class="sourceLine" id="cb3-2" title="2">sigma_sq <span class="op">=</span> [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">10</span>, <span class="dv">100</span>]</a>
<a class="sourceLine" id="cb3-3" title="3">fig,ax <span class="op">=</span> plt.subplots(nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">15</span>,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb3-4" title="4">plt_ind <span class="op">=</span> np.arange(<span class="dv">4</span>) <span class="op">+</span> <span class="dv">141</span></a>
<a class="sourceLine" id="cb3-5" title="5"><span class="cf">for</span> s, ind <span class="kw">in</span> <span class="bu">zip</span>(sigma_sq, plt_ind):</a>
<a class="sourceLine" id="cb3-6" title="6">    plt.subplot(ind)</a>
<a class="sourceLine" id="cb3-7" title="7">    f <span class="op">=</span> np.exp(<span class="op">-</span>distance <span class="op">**</span> <span class="dv">2</span> <span class="op">/</span> <span class="dv">2</span> <span class="op">/</span> s)</a>
<a class="sourceLine" id="cb3-8" title="8">    plt.plot(distance, f, c<span class="op">=</span><span class="st">&#39;cyan&#39;</span>)</a>
<a class="sourceLine" id="cb3-9" title="9">    plt.title(<span class="st">&#39;$\sigma^2$ = &#39;</span> <span class="op">+</span> <span class="bu">str</span>(s))</a>
<a class="sourceLine" id="cb3-10" title="10">    plt.xlabel(<span class="st">&#39;Distance&#39;</span>)</a>
<a class="sourceLine" id="cb3-11" title="11">    plt.ylabel(<span class="st">&#39;Neighborhood function $f$&#39;</span>)</a>
<a class="sourceLine" id="cb3-12" title="12">fig.subplots_adjust(hspace<span class="op">=</span><span class="fl">0.5</span>, wspace<span class="op">=</span><span class="fl">0.3</span>)</a>
<a class="sourceLine" id="cb3-13" title="13">plt.show()</a></code></pre></div>
<figure>
<img src="https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-2.png" alt="decay rate for self organizing maps" /><figcaption>decay rate for self organizing maps</figcaption>
</figure>
<h4 id="implementing-a-self-organizing-map-in-python-using-numpy">Implementing a Self-Organizing Map in Python Using NumPy <a id="implementingaselforganizingmapinpythonusingnumpy"></a></h4>
<p>As there is no built-in routine for an SOM in the de-facto standard machine learning library, <em><strong>Scikit-Learn</strong></em>, we’ll do a quick implementation manually using <em><strong>NumPy</strong></em>. The unsupervised machine learning model is pretty straightforward and easy to implement.</p>
<p>We’ll implement the SOM as a 2D <code>mxn</code> grid, hence requiring a 3D <code>NumPy</code> array. The third dimension is required for storing the weights in each cell:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1"><span class="co"># Return the (g,h) index of the BMU in the grid</span></a>
<a class="sourceLine" id="cb4-2" title="2"><span class="kw">def</span> find_BMU(SOM,x):</a>
<a class="sourceLine" id="cb4-3" title="3">    distSq <span class="op">=</span> (np.square(SOM <span class="op">-</span> x)).<span class="bu">sum</span>(axis<span class="op">=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb4-4" title="4">    <span class="cf">return</span> np.unravel_index(np.argmin(distSq, axis<span class="op">=</span><span class="va">None</span>), distSq.shape)</a>
<a class="sourceLine" id="cb4-5" title="5"></a>
<a class="sourceLine" id="cb4-6" title="6"><span class="co"># Update the weights of the SOM cells when given a single training example</span></a>
<a class="sourceLine" id="cb4-7" title="7"><span class="co"># and the model parameters along with BMU coordinates as a tuple</span></a>
<a class="sourceLine" id="cb4-8" title="8"><span class="kw">def</span> update_weights(SOM, train_ex, learn_rate, radius_sq,</a>
<a class="sourceLine" id="cb4-9" title="9">                   BMU_coord, step<span class="op">=</span><span class="dv">3</span>):</a>
<a class="sourceLine" id="cb4-10" title="10">    g, h <span class="op">=</span> BMU_coord</a>
<a class="sourceLine" id="cb4-11" title="11">    <span class="co">#if radius is close to zero then only BMU is changed</span></a>
<a class="sourceLine" id="cb4-12" title="12">    <span class="cf">if</span> radius_sq <span class="op">&lt;</span> <span class="fl">1e-3</span>:</a>
<a class="sourceLine" id="cb4-13" title="13">        SOM[g,h,:] <span class="op">+=</span> learn_rate <span class="op">*</span> (train_ex <span class="op">-</span> SOM[g,h,:])</a>
<a class="sourceLine" id="cb4-14" title="14">        <span class="cf">return</span> SOM</a>
<a class="sourceLine" id="cb4-15" title="15">    <span class="co"># Change all cells in a small neighborhood of BMU</span></a>
<a class="sourceLine" id="cb4-16" title="16">    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">max</span>(<span class="dv">0</span>, g<span class="op">-</span>step), <span class="bu">min</span>(SOM.shape[<span class="dv">0</span>], g<span class="op">+</span>step)):</a>
<a class="sourceLine" id="cb4-17" title="17">        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">max</span>(<span class="dv">0</span>, h<span class="op">-</span>step), <span class="bu">min</span>(SOM.shape[<span class="dv">1</span>], h<span class="op">+</span>step)):</a>
<a class="sourceLine" id="cb4-18" title="18">            dist_sq <span class="op">=</span> np.square(i <span class="op">-</span> g) <span class="op">+</span> np.square(j <span class="op">-</span> h)</a>
<a class="sourceLine" id="cb4-19" title="19">            dist_func <span class="op">=</span> np.exp(<span class="op">-</span>dist_sq <span class="op">/</span> <span class="dv">2</span> <span class="op">/</span> radius_sq)</a>
<a class="sourceLine" id="cb4-20" title="20">            SOM[i,j,:] <span class="op">+=</span> learn_rate <span class="op">*</span> dist_func <span class="op">*</span> (train_ex <span class="op">-</span> SOM[i,j,:])</a>
<a class="sourceLine" id="cb4-21" title="21">    <span class="cf">return</span> SOM</a>
<a class="sourceLine" id="cb4-22" title="22"></a>
<a class="sourceLine" id="cb4-23" title="23"><span class="co"># Main routine for training an SOM. It requires an initialized SOM grid</span></a>
<a class="sourceLine" id="cb4-24" title="24"><span class="co"># or a partially trained grid as parameter</span></a>
<a class="sourceLine" id="cb4-25" title="25"><span class="kw">def</span> train_SOM(SOM, train_data, learn_rate <span class="op">=</span> <span class="fl">.1</span>, radius_sq <span class="op">=</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb4-26" title="26">             lr_decay <span class="op">=</span> <span class="fl">.1</span>, radius_decay <span class="op">=</span> <span class="fl">.1</span>, epochs <span class="op">=</span> <span class="dv">10</span>):</a>
<a class="sourceLine" id="cb4-27" title="27">    learn_rate_0 <span class="op">=</span> learn_rate</a>
<a class="sourceLine" id="cb4-28" title="28">    radius_0 <span class="op">=</span> radius_sq</a>
<a class="sourceLine" id="cb4-29" title="29">    <span class="cf">for</span> epoch <span class="kw">in</span> np.arange(<span class="dv">0</span>, epochs):</a>
<a class="sourceLine" id="cb4-30" title="30">        rand.shuffle(train_data)</a>
<a class="sourceLine" id="cb4-31" title="31">        <span class="cf">for</span> train_ex <span class="kw">in</span> train_data:</a>
<a class="sourceLine" id="cb4-32" title="32">            g, h <span class="op">=</span> find_BMU(SOM, train_ex)</a>
<a class="sourceLine" id="cb4-33" title="33">            SOM <span class="op">=</span> update_weights(SOM, train_ex,</a>
<a class="sourceLine" id="cb4-34" title="34">                                 learn_rate, radius_sq, (g,h))</a>
<a class="sourceLine" id="cb4-35" title="35">        <span class="co"># Update learning rate and radius</span></a>
<a class="sourceLine" id="cb4-36" title="36">        learn_rate <span class="op">=</span> learn_rate_0 <span class="op">*</span> np.exp(<span class="op">-</span>epoch <span class="op">*</span> lr_decay)</a>
<a class="sourceLine" id="cb4-37" title="37">        radius_sq <span class="op">=</span> radius_0 <span class="op">*</span> np.exp(<span class="op">-</span>epoch <span class="op">*</span> radius_decay)</a>
<a class="sourceLine" id="cb4-38" title="38">    <span class="cf">return</span> SOM</a></code></pre></div>
<p>Let’s break down the key functions used to implement a Self-Organizing Map:</p>
<p><code>find_BMU()</code> returns the grid cell coordinates of the best matching unit when given the <code>SOM</code> grid and a training example <code>x</code>. It computes the square of the Euclidean distance between each cell weight and <code>x</code>, and returns <code>(g,h)</code>, i.e., the cell coordinates with the minimum distance.</p>
<p>The <code>update_weights()</code> function requires an SOM grid, a training example <code>x</code>, the parameters <code>learn_rate</code> and <code>radius_sq</code>, the coordinates of the best matching unit, and a <code>step</code> parameter. Theoretically, all cells of the SOM are updated on the next training example. However, we showed earlier that the change is negligible for cells that are far away from the BMU. Hence, we can make the code more efficient by changing only the cells in a small vicinity of the BMU. The <code>step</code> parameter specifies the maximum number of cells on the left, right, above, and below to change when updating the weights.</p>
<p>Finallt, the <code>train_SOM()</code> function implements the main training procedure of an SOM. It requires an initialized or partially trained <code>SOM</code> grid and <code>train_data</code> as parameters. The advantage is to be able to train the SOM from a previous trained stage. Additionally <code>learn_rate</code> and <code>radius_sq</code> parameters are required along with their corresponding decay rates <code>lr_decay</code> and <code>radius_decay</code>. The <code>epochs</code> parameter is set to 10 by default but can be changed if needed.</p>
<h4 id="running-the-self-organizing-map-on-a-practical-example">Running the Self-Organizing Map on a Practical Example <a id="runningtheselforganizingmaponapracticalexample"></a></h4>
<p>One of the commonly cited examples for training an SOM is that of random colors. We can train an SOM grid and easily visualize how various similar colors get arranged in neighboring cells.</p>
<blockquote>
<p>Cells far away from each other have different colors.</p>
</blockquote>
<p>Let’s run the <code>train_SOM()</code> function on a training data matrix filled with random RGB colors.</p>
<p>The code below initializes a training data matrix and an SOM grid with random RGB colors. It also displays the training data and the randomly initialized <em>SOM grid</em>. Note, the training matrix is a 3000x3 matrix, however, we have reshaped it to 50x60x3 matrix for visualization:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1"><span class="co"># Dimensions of the SOM grid</span></a>
<a class="sourceLine" id="cb5-2" title="2">m <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb5-3" title="3">n <span class="op">=</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb5-4" title="4"><span class="co"># Number of training examples</span></a>
<a class="sourceLine" id="cb5-5" title="5">n_x <span class="op">=</span> <span class="dv">3000</span></a>
<a class="sourceLine" id="cb5-6" title="6">rand <span class="op">=</span> np.random.RandomState(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb5-7" title="7"><span class="co"># Initialize the training data</span></a>
<a class="sourceLine" id="cb5-8" title="8">train_data <span class="op">=</span> rand.randint(<span class="dv">0</span>, <span class="dv">255</span>, (n_x, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb5-9" title="9"><span class="co"># Initialize the SOM randomly</span></a>
<a class="sourceLine" id="cb5-10" title="10">SOM <span class="op">=</span> rand.randint(<span class="dv">0</span>, <span class="dv">255</span>, (m, n, <span class="dv">3</span>)).astype(<span class="bu">float</span>)</a>
<a class="sourceLine" id="cb5-11" title="11"><span class="co"># Display both the training matrix and the SOM grid</span></a>
<a class="sourceLine" id="cb5-12" title="12">fig, ax <span class="op">=</span> plt.subplots(</a>
<a class="sourceLine" id="cb5-13" title="13">    nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">2</span>, figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="fl">3.5</span>),</a>
<a class="sourceLine" id="cb5-14" title="14">    subplot_kw<span class="op">=</span><span class="bu">dict</span>(xticks<span class="op">=</span>[], yticks<span class="op">=</span>[]))</a>
<a class="sourceLine" id="cb5-15" title="15">ax[<span class="dv">0</span>].imshow(train_data.reshape(<span class="dv">50</span>, <span class="dv">60</span>, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb5-16" title="16">ax[<span class="dv">0</span>].title.set_text(<span class="st">&#39;Training Data&#39;</span>)</a>
<a class="sourceLine" id="cb5-17" title="17">ax[<span class="dv">1</span>].imshow(SOM.astype(<span class="bu">int</span>))</a>
<a class="sourceLine" id="cb5-18" title="18">ax[<span class="dv">1</span>].title.set_text(<span class="st">&#39;Randomly Initialized SOM Grid&#39;</span>)</a></code></pre></div>
<figure>
<img src="https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-3.png" alt="randomly initialized self organizing map" /><figcaption>randomly initialized self organizing map</figcaption>
</figure>
<p>Let’s now train the SOM and check up on it every 5 epochs as a quick overview of its progress:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb6-1" title="1">fig, ax <span class="op">=</span> plt.subplots(</a>
<a class="sourceLine" id="cb6-2" title="2">    nrows<span class="op">=</span><span class="dv">1</span>, ncols<span class="op">=</span><span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="fl">3.5</span>),</a>
<a class="sourceLine" id="cb6-3" title="3">    subplot_kw<span class="op">=</span><span class="bu">dict</span>(xticks<span class="op">=</span>[], yticks<span class="op">=</span>[]))</a>
<a class="sourceLine" id="cb6-4" title="4">total_epochs <span class="op">=</span> <span class="dv">0</span></a>
<a class="sourceLine" id="cb6-5" title="5"><span class="cf">for</span> epochs, i <span class="kw">in</span> <span class="bu">zip</span>([<span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">10</span>], <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">4</span>)):</a>
<a class="sourceLine" id="cb6-6" title="6">    total_epochs <span class="op">+=</span> epochs</a>
<a class="sourceLine" id="cb6-7" title="7">    SOM <span class="op">=</span> train_SOM(SOM, train_data, epochs<span class="op">=</span>epochs)</a>
<a class="sourceLine" id="cb6-8" title="8">    ax[i].imshow(SOM.astype(<span class="bu">int</span>))</a>
<a class="sourceLine" id="cb6-9" title="9">    ax[i].title.set_text(<span class="st">&#39;Epochs = &#39;</span> <span class="op">+</span> <span class="bu">str</span>(total_epochs))</a></code></pre></div>
<figure>
<img src="https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-4.png" alt="self organizing map training and results" /><figcaption>self organizing map training and results</figcaption>
</figure>
<p>The example above is very interesting as it shows how the grid automatically arranges the RGB colors so that various shades of the same color are close together in the SOM grid. The arrangement takes place as early as the first epoch, but it’s not ideal. We can see that the SOM converges in around 10 epochs and there are fewer changes in the subsequent epochs.</p>
<p><strong>Effect of Learning Rate and Radius</strong></p>
<p>To see how the learning rate varies for different learning rates and radii, we can run the SOM for 10 epochs when starting from the same initial grid. The code below trains the SOM for three different values of the learning rate and three different radii.</p>
<p>The SOM is rendered after 5 epochs for each simulation:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb7-1" title="1">fig, ax <span class="op">=</span> plt.subplots(</a>
<a class="sourceLine" id="cb7-2" title="2">    nrows<span class="op">=</span><span class="dv">3</span>, ncols<span class="op">=</span><span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">15</span>),</a>
<a class="sourceLine" id="cb7-3" title="3">    subplot_kw<span class="op">=</span><span class="bu">dict</span>(xticks<span class="op">=</span>[], yticks<span class="op">=</span>[]))</a>
<a class="sourceLine" id="cb7-4" title="4"></a>
<a class="sourceLine" id="cb7-5" title="5"><span class="co"># Initialize the SOM randomly to the same state</span></a>
<a class="sourceLine" id="cb7-6" title="6"></a>
<a class="sourceLine" id="cb7-7" title="7"><span class="cf">for</span> learn_rate, i <span class="kw">in</span> <span class="bu">zip</span>([<span class="fl">0.001</span>, <span class="fl">0.5</span>, <span class="fl">0.99</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]):</a>
<a class="sourceLine" id="cb7-8" title="8">    <span class="cf">for</span> radius_sq, j <span class="kw">in</span> <span class="bu">zip</span>([<span class="fl">0.01</span>, <span class="dv">1</span>, <span class="dv">10</span>], [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">2</span>]):</a>
<a class="sourceLine" id="cb7-9" title="9">        rand <span class="op">=</span> np.random.RandomState(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb7-10" title="10">        SOM <span class="op">=</span> rand.randint(<span class="dv">0</span>, <span class="dv">255</span>, (m, n, <span class="dv">3</span>)).astype(<span class="bu">float</span>)</a>
<a class="sourceLine" id="cb7-11" title="11">        SOM <span class="op">=</span> train_SOM(SOM, train_data, epochs <span class="op">=</span> <span class="dv">5</span>,</a>
<a class="sourceLine" id="cb7-12" title="12">                        learn_rate <span class="op">=</span> learn_rate,</a>
<a class="sourceLine" id="cb7-13" title="13">                        radius_sq <span class="op">=</span> radius_sq)</a>
<a class="sourceLine" id="cb7-14" title="14">        ax[i][j].imshow(SOM.astype(<span class="bu">int</span>))</a>
<a class="sourceLine" id="cb7-15" title="15">        ax[i][j].title.set_text(<span class="st">&#39;$\eta$ = &#39;</span> <span class="op">+</span> <span class="bu">str</span>(learn_rate) <span class="op">+</span></a>
<a class="sourceLine" id="cb7-16" title="16">                                <span class="st">&#39;, $\sigma^2$ = &#39;</span> <span class="op">+</span> <span class="bu">str</span>(radius_sq))</a></code></pre></div>
<figure>
<img src="https://s3.stackabuse.com/media/articles/self-organizing-maps-theory-and-implementation-in-python-with-numpy-5.png" alt="effects and tuning self organizing map hyperparameters" /><figcaption>effects and tuning self organizing map hyperparameters</figcaption>
</figure>
<p>The example above shows that for radius values close to zero (first column), the SOM only changes the individual cells but not the neighboring cells. Hence, a proper map is not created regardless of the learning rate. A similar case is also encountered for smaller learning rates (first row, second column). As with any other machine learning algorithm, a good balance of parameters is required for ideal training.</p>
<h4 id="conclusions">Conclusions <a id="conclusions"></a></h4>
<p>In this guide, we discussed the theoretical model of an SOM and its detailed implementation. We demonstrated the SOM on RGB colors and showed how different shades of the same color organized themselves on a 2D grid.</p>
<p>While the SOMs are no longer very popular in the machine learning community, they remain a good model for data summary and visualization.</p>
</body>
</html>
